# ollama
replicaCount: 1

image:
  repository: docker.io/ollama/ollama
  pullPolicy: IfNotPresent
  tag: latest

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  automount: true
  annotations: {}
  name: ""

podAnnotations: {}
podLabels: {}

podSecurityContext: {}

securityContext: {}

service:
  type: ClusterIP
  port: 11434

resources:
  requests:
    cpu: "2"
    memory: "2Gi"
    #nvidia.com/gpu: 1

  limits:
    cpu: "8" 
    memory: "12Gi"
    #nvidia.com/gpu: 1


livenessProbe:
  tcpSocket:
    port: ollama
readinessProbe:
  tcpSocket:
    port: ollama

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80

volumes: []

volumeMounts: []

nodeSelector: {}

tolerations: []

affinity: {}

storage:
  size: 40Gi

proxy:
  enabled: False
  http: ""
  https: ""
  noProxy: ""

openweb:
  replicaCount: 1

  image:
    repository: ghcr.io/open-webui/open-webui
    pullPolicy: IfNotPresent
    tag: main

  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""

  serviceAccount:
    create: true
    automount: true
    annotations: {}
    name: ""

  podAnnotations: {}
  podLabels: {}

  podSecurityContext: {}

  securityContext: {}

  service:
    type: ClusterIP
    port: 8080

  resources:
    requests:
      cpu: "1"
      memory: "1Gi"
    limits:
      cpu: "2" 
      memory: "4Gi"

  livenessProbe:
    httpGet:
      path: /
      port: http
  readinessProbe:
    httpGet:
      path: /
      port: http

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80

  volumes: []

  volumeMounts: []

  nodeSelector: {}

  tolerations: []

  affinity: {}

vllm:
  enabled: False
  replicaCount: 1

  image:
    repository: vllm/vllm-openai
    pullPolicy: IfNotPresent
    tag: latest

  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""

  serviceAccount:
    create: true
    automount: true
    annotations: {}
    name: ""

  podAnnotations: {}
  podLabels: {}

  podSecurityContext: {}

  securityContext: {}

  service:
    type: ClusterIP
    port: 8000

  resources:
    requests:
      cpu: "2"
      memory: "2Gi"
      #nvidia.com/gpu: 1

    limits:
      cpu: "8" 
      memory: "12Gi"
      #nvidia.com/gpu: 1

  livenessProbe:
    httpGet:
      path: /health
      port: http
  readinessProbe:
    httpGet:
      path: /health
      port: http

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80

  volumes: []

  volumeMounts: []

  nodeSelector: {}

  tolerations: []

  affinity: {}

  storage:
    size: 40Gi
  
  args: "vllm serve mistralai/Mistral-7B-Instruct-v0.3 --trust-remote-code --enable-chunked-prefill --max_num_batched_tokens 1024"