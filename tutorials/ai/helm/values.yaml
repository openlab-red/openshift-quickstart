# Global
imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""
aiType: "ollama"  # Default AI type to use for helpers


# ollama
ollama:
  replicaCount: 1

  image:
    repository: docker.io/ollama/ollama
    pullPolicy: IfNotPresent
    tag: latest

  imagePullSecrets: []

  serviceAccount:
    create: true
    automount: true
    annotations: {}
    name: ""

  podAnnotations: {}
  podLabels: {}

  podSecurityContext: {}

  securityContext: {}

  service:
    type: ClusterIP
    port: 11434

  resources:
    requests:
      cpu: "2"
      memory: "2Gi"
      #nvidia.com/gpu: 1

    limits:
      cpu: "8"
      memory: "12Gi"
      #nvidia.com/gpu: 1

  livenessProbe:
    tcpSocket:
      port: ollama
    initialDelaySeconds: 30
  readinessProbe:
    tcpSocket:
      port: ollama
    initialDelaySeconds: 30

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80

  volumes: []

  volumeMounts: []

  nodeSelector: {}

  tolerations: []

  affinity: {}

  storage:
    size: 40Gi

proxy:
  enabled: False
  http: ""
  https: ""
  noProxy: ""

openweb:
  replicaCount: 1

  image:
    repository: ghcr.io/open-webui/open-webui
    pullPolicy: IfNotPresent
    tag: main

  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""

  serviceAccount:
    create: true
    automount: true
    annotations: {}
    name: ""

  podAnnotations: {}
  podLabels: {}

  podSecurityContext: {}

  securityContext: {}

  service:
    type: ClusterIP
    port: 8080

  resources:
    requests:
      cpu: "1"
      memory: "1Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

  livenessProbe:
    httpGet:
      path: /health
      port: http
    initialDelaySeconds: 30
  readinessProbe:
    httpGet:
      path: /health
      port: http
    initialDelaySeconds: 30

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80

  volumes: []

  volumeMounts: []

  nodeSelector: {}

  tolerations: []

  affinity: {}

# ramalama
ramalama:
  replicaCount: 1

  image:
    repository: quay.io/ramalama/ramalama
    pullPolicy: IfNotPresent
    tag: latest

  imagePullSecrets: []

  serviceAccount:
    create: true
    automount: true
    annotations: {}
    name: ""

  podAnnotations: {}
  podLabels: {}

  podSecurityContext: {}

  securityContext: {}

  service:
    type: ClusterIP
    port: 8080

  resources:
    requests:
      cpu: "2"
      memory: "2Gi"
      #nvidia.com/gpu: 1

    limits:
      cpu: "8"
      memory: "12Gi"
      #nvidia.com/gpu: 1

  livenessProbe:
    tcpSocket:
      port: ramalama
    initialDelaySeconds: 30
  readinessProbe:
    tcpSocket:
      port: ramalama
    initialDelaySeconds: 30

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80

  volumes: []

  volumeMounts: []

  nodeSelector: {}

  tolerations: []

  affinity: {}

  storage:
    size: 40Gi

vllm:
  replicaCount: 1

  image:
    repository: vllm/vllm-openai
    pullPolicy: IfNotPresent
    tag: latest

  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""

  serviceAccount:
    create: true
    automount: true
    annotations: {}
    name: ""

  podAnnotations: {}
  podLabels: {}

  podSecurityContext: {}

  securityContext: {}

  service:
    type: ClusterIP
    port: 8000

  resources:
    requests:
      cpu: "2"
      memory: "2Gi"
      #nvidia.com/gpu: 1

    limits:
      cpu: "8"
      memory: "12Gi"
      #nvidia.com/gpu: 1

  livenessProbe:
    httpGet:
      path: /health
      port: http
  readinessProbe:
    httpGet:
      path: /health
      port: http

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80

  volumes: []

  volumeMounts: []

  nodeSelector: {}

  tolerations: []

  affinity: {}

  storage:
    size: 40Gi

  args: "vllm serve mistralai/Mistral-7B-Instruct-v0.3 --trust-remote-code --enable-chunked-prefill --max_num_batched_tokens 1024"